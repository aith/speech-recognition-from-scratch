{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "pycharm-5af6fa8",
      "language": "python",
      "display_name": "PyCharm (001_game_of_life)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "index.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eMIen1MlqYMi"
      },
      "source": [
        "In this notebook, I'll provide an wide overview on Automatic Speech Architecture, its standard pipelines, and how to train a model for it!\n",
        "\n",
        "### What is Automatic Speech Recognition?\n",
        "Automatic Speech Recognition (ASR) is the translation of spoken speech to text by a computer.\n",
        "\n",
        "It has many uses:\n",
        "+ closed captioning\n",
        "+ mobile phone voice assistants\n",
        "+ interface for handicapped individuals\n",
        "+ preserve endangered languages\n",
        "\n",
        "However, ASR is very difficult to solve due to:\n",
        "+ Environment (Is there noise? Other speakers?)\n",
        "+ Style of speech (Is it casual, formal, poetic, etc?)\n",
        "+ Style of speaker (Talks fast? Accents? Gender bias?)\n",
        "\n",
        "https://www.youtube.com/watch?v=q67z7PTGRi8\n",
        "\n",
        "Modern ASR models work quite well in ideal, noiseless conditions. However, in noisy conditions they are far from perfect. Below, we can see the typical accuracy for state-of-the-art ASR models in 2020!\n",
        "\n",
        "(Note: To calculate the accuracy of ASR models, we use a 'Word Error Rate.' This determines which percentage of spoken words were incorrectly translated to text by the ASR model. )\n",
        "\n",
        "![word error rate](https://drive.google.com/uc?export=view&id=1iUz9koIqQErBVB0a4db0gSZVdfsfwh9L)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eUVn2wDxqYMj"
      },
      "source": [
        "\n",
        "Architecture:\n",
        "\n",
        "![timeline](https://drive.google.com/uc?export=view&id=1n2ID2ML5x9TFSBJhfdvj5AGI4u-9Mp4m)\n",
        "\n",
        "\n",
        "From the 1980s to the 2010s, ASR was predominately performed by Hidden Markov Model (HMM) architecture. HMMs are probabilistic models for linear sequence classification problems. Given text's linear nature, HMMs seem like a natural fit for ASR.\n",
        "However, using HMMs for ASR come with 3 downsides:\n",
        "+ HMMs are built on the Markov Assumption, which assumes that the probability of the next state only depends on the current state, and not any states prior. This doesn't make sense for language processing, which is highly contextual (see Coarticulation)\n",
        "+ HMMs' state transitions probabilities are 'baked in' and thus inflexible to changes in language\n",
        "+ Classic HMM ASR pipelines require hand-tuned probability distributions for accoustic models and language models from linguistic experts\n",
        "+ HMM-based ASR architecture is very complex, requiring 3 models as input. The figure below demonstrates an HMM-centric ASR architecture\n",
        "\n",
        "![an HMM-based architecture](https://drive.google.com/uc?export=view&id=1loF8wbD-6DRO45Ly7lKnVVIcCn83H-B9)\n",
        "\n",
        "Nowadays Recurrent Neural Networks (RNNs) or RNN-HMM hybrids are favored for ASR because they solve these problems. However, RNN's come at the cost of requiring a vast amount of training data.\n",
        "\n",
        "![an RNN-only architecture](https://drive.google.com/uc?export=view&id=1_9McwMlHIqlPuJGYOkPwB-8h-t13P4pI)\n",
        "(an RNN-only model that has only 4 steps)\n",
        "\n",
        "https://stats.stackexchange.com/questions/282987/hidden-markov-model-vs-recurrent-neural-network\n",
        "Jurafsky textbook\n",
        "\n",
        "For the our ASR model, we'll be using an 'Listen, Attend, and Spell' (LAS) RNN. The LAS was proposed by in 2016 for ASR architecture in order to simplify Hybrid architecture (such as DNN-HMMs or CTC-HMMs) and avoid the aforementioned independence assumptions. Since then it has been the forefront of ASR architecture.\n",
        "\n",
        "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44926.pdf\n",
        "\n",
        "The objective of LAS is to transform an arbitrary-length input sequence to an arbitrary-length output sequence. LAS is an encoder-decoder model- a class of RNN's that is well-suited for problems where the length of the inputs greatly differs from the length of the outputs (like ASR).\n",
        "\n",
        "Encoder-Decoder models are composed of 3 parts:\n",
        "+ an Encoder\n",
        "+ a Context Vector\n",
        "+ a Decoder\n",
        "\n",
        "The Encoder compresses and summarizes input data into a Context Vector, which is later transformed by the Decoder. The Context Vector must be defined with a fixed-length (usually 256, 512, or 1024).\n",
        "\n",
        "Accoustic Information as Input for RNN:\n",
        "A classic HMM-based ASR model would require us to input 3 probabilistic models (accoustic, pronunciation, and language), whereas an RNN only requires 1 (accoustic). The accoustic model captures important characteristics about how we perceive speech (see MFCC). We'll extract this data from the LibriSpeech ASR dataset.\n",
        "\n",
        "https://www.openslr.org/12\n",
        "\n",
        "We need to first extract features from the audio files- we'll be putting these features in a vector and feeding them into the RNN.\n",
        "In order to extract the features from a given file, we need to perform the following steps:\n",
        "```\n",
        "1) Convert the FLAC file into a Waveform (giving us frequency data over the time domain)\n",
        "2) Split the Waveform into Windows using a Windowing Function (like Hamming)\n",
        "3) For each Window:\n",
        "    a) Compute its Fourier Transform\n",
        "    b) Compute its Mel Spectrogram (also known as Mel Filter Bank)\n",
        "    c) Compute the Log of each value in the Mel Spectrogram\n",
        "    d) Append the result of 3c to the Features Vector\n",
        "4) Return Features Vector\n",
        "```\n",
        "\n",
        "But before we do any computations, let's define some configuration constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AXrfGRTpqYMl"
      },
      "source": [
        "sample_rate = 16000  # The Sample Rate of the LibriSpeech Dataset\n",
        "window_size_frames = int(sample_rate * 0.025)  # The standard window size for ASR models is 25ms\n",
        "window_step_frames = int(sample_rate * 0.010) # The standard window step for ASR models is 10ms\n",
        "n_filters = 40"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDeN6xz5wvDY"
      },
      "source": [
        "And of course, download the dataset(s):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5xKocQIsvmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777a6620-b72c-4056-9f8c-762a49338f33"
      },
      "source": [
        "DATASET_DIR = './data/'\n",
        "\n",
        "DATASETS = [\n",
        "  \"dev-clean\"\n",
        "  # \"train-clean-100\"\n",
        "  # \"train-clean-360\"  # this exceeds Colab's Disk limit\n",
        "  # \"train-clean-500\"  # this exceeds Colab's Disk limit\n",
        "]\n",
        "\n",
        "def download_datasets(datasets): \n",
        "  for dataset in datasets:\n",
        "    dest = DATASET_DIR + dataset + '.tar.gz'\n",
        "    !mkdir -p {DATASET_DIR}\n",
        "    !wget -O {dest} https://www.openslr.org/resources/12/{dataset}.tar.gz\n",
        "\n",
        "def extract_datasets(datasets):\n",
        "  for dataset in datasets:\n",
        "    src = DATASET_DIR + dataset + \".tar.gz\"\n",
        "    dest = DATASET_DIR + dataset + '/'\n",
        "    !mkdir -p {DATASET_DIR + dataset}\n",
        "    !tar -xf {src} -C {dest}\n",
        "    print(\"Finished extraction\")\n",
        "\n",
        "download_datasets(DATASETS)\n",
        "extract_datasets(DATASETS)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-06 20:02:27--  https://www.openslr.org/resources/12/dev-clean.tar.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 337926286 (322M) [application/x-gzip]\n",
            "Saving to: ‘./data/dev-clean.tar.gz’\n",
            "\n",
            "./data/dev-clean.ta 100%[===================>] 322.27M  25.9MB/s    in 13s     \n",
            "\n",
            "2021-06-06 20:02:41 (24.1 MB/s) - ‘./data/dev-clean.tar.gz’ saved [337926286/337926286]\n",
            "\n",
            "Finished extraction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "cShnG7OEqYMm"
      },
      "source": [
        "Now let's get the Log Mel Filter Bank energies! We'll be using the speechpy library for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e390m14sHn-z"
      },
      "source": [
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "!pip install speechpy\n",
        "import speechpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Uq6BRWyO0jjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03909609-258d-4827-ed9a-025051b980a5"
      },
      "source": [
        "\n",
        "def files_to_log_mel_spec(paths, sample_rate, window_size_frames, window_step_frames, num_mel_bins):\n",
        "  \"\"\" Returns \n",
        "  ( Tensor of Log Mel Spectrograms of the audio files in paths, \n",
        "    numpy array of the Lengths of the Spectrograms ) \n",
        "  \"\"\"\n",
        "  lmspecs = [] \n",
        "  lengths = []\n",
        "  for path in paths:\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = librosa.load(path, sample_rate)\n",
        "    audio = tf.convert_to_tensor(audio)\n",
        "    # Get Spectrogram using Short-time Fourier Transform\n",
        "    stfts = tf.signal.stft(audio, frame_length=window_size_frames, frame_step=window_step_frames, fft_length=1024)\n",
        "    spectrograms = tf.abs(stfts)\n",
        "    # Warp the linear scale spectrograms into the Mel Scale.\n",
        "    num_spectrogram_bins = stfts.shape[-1]\n",
        "    lower_edge_hertz, upper_edge_hertz = 80.0, 7600.0\n",
        "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "      num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
        "      upper_edge_hertz)\n",
        "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n",
        "      linear_to_mel_weight_matrix.shape[-1:]))\n",
        "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
        "    lmspec = tf.math.log(mel_spectrograms + 1e-6)\n",
        "    lmspecs.append(lmspec)\n",
        "    lengths.append(len(lmspec))\n",
        "  # Normalize all\n",
        "  means = tf.math.reduce_mean(lmspecs, 1, keepdims=True)\n",
        "  stddevs = tf.math.reduce_std(lmspecs, 1, keepdims=True)\n",
        "  lmspecs = (lmspecs - means) / stddevs\n",
        "  audio_len = tf.shape(lmspecs)[0]\n",
        "  # padding to 10 seconds\n",
        "  # pad_len = 2754\n",
        "  # paddings = tf.constant([[0, pad_len], [0, 0]])\n",
        "  # lmspec = tf.pad(lmspec, paddings, \"CONSTANT\")[:pad_len, :]\n",
        "  return lmspecs, np.array(lengths).astype(np.int32)\n",
        "\n",
        "files_to_log_mel_spec(['./data/dev-clean/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac'], sample_rate, window_size_frames, window_step_frames, n_filters)\n",
        "\n",
        "# def files_to_log_mel_spec(filepaths, sample_rate, window_size, window_step, n_filters):  # LibriSpeech uses a 16kHz sample rate\n",
        "#     \"\"\" Convert an audio file to a Log Mel Spectrogram \"\"\"\n",
        "#     log_mels = []\n",
        "#     lengths = []\n",
        "#     for filepath in filepaths:\n",
        "#         test_sig, _ = librosa.load(filepath, sample_rate)\n",
        "#         # Perform Mel Spec\n",
        "#         # mel_spec = librosa.feature.melspectrogram(test_sig, sr=sample_rate, n_fft=window_size, hop_length=window_step, window='hann')\n",
        "#         # log_mel = np.log(mel_spec)\n",
        "#         log_mel = speechpy.feature.lmfe(test_sig, sample_rate, frame_length=0.025, frame_stride=0.010, num_filters=n_filters).astype(np.float32)\n",
        "#         # log_mel = np.asarray(log_mel, dtype=np.float)\n",
        "#         # print(log_mel.shape)\n",
        "#         # Log it\n",
        "#         log_mels.append(log_mel)\n",
        "#         lengths.append(len(log_mels))\n",
        "#     # print(log_mels)\n",
        "#     log_mels = np.array(log_mels)\n",
        "#     # print(log_mels.shape)\n",
        "#     # normalize it\n",
        "#     means = tf.math.reduce_mean(log_mels, 1, keepdims=True)\n",
        "#     stddevs = tf.math.reduce_std(log_mels, 1, keepdims=True)\n",
        "#     # print(log_mels)\n",
        "#     # try:\n",
        "#     #   means = np.mean(log_mels,axis=1,keepdims=True)  \n",
        "#     # except AxisError:\n",
        "#     #   print(log_mels.shape)\n",
        "#     # stddevs = np.std(log_mels,axis=1,keepdims=True)\n",
        "#     normalized = (log_mels - means) / stddevs\n",
        "#     return normalized, np.array(lengths).astype(np.int32)\n",
        "\n",
        "# Example\n",
        "# files_to_log_mel_spec(['././data/dev-clean/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac'], sample_rate, window_size, window_step, n_filters)\n",
        "# test = np.array((1,2))\n",
        "# tf.reduce_mean(test)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 584, 40), dtype=float32, numpy=\n",
              " array([[[-1.5120794 , -1.6673089 , -1.8040851 , ..., -1.566706  ,\n",
              "          -1.4199985 , -1.4137021 ],\n",
              "         [-1.6922767 , -1.7004411 , -1.5005974 , ..., -1.4650229 ,\n",
              "          -1.4228181 , -1.3736689 ],\n",
              "         [-1.3246887 , -1.3394393 , -1.3179259 , ..., -1.448277  ,\n",
              "          -1.4855719 , -1.4314622 ],\n",
              "         ...,\n",
              "         [-1.558339  , -1.6693529 , -1.4325359 , ..., -1.3655134 ,\n",
              "          -1.0403051 , -0.98255634],\n",
              "         [-1.4902389 , -1.6166751 , -1.520029  , ..., -1.36802   ,\n",
              "          -0.9872307 , -0.9673391 ],\n",
              "         [-1.516491  , -1.4677212 , -1.3451515 , ..., -1.5840408 ,\n",
              "          -0.93954355, -0.90887135]]], dtype=float32)>,\n",
              " array([584], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9ytf-ldeqYMo"
      },
      "source": [
        "Sometimes you may see MFCCs used as the ASR features instead of Mel Filter Banks. MFCCs are values that loosely represents the brain's capacity to filter out certain signals. MFCCs were popular with HMM-based models for helping model based on human-perceptible information, but newer architecture can usually use Filter Banks and get similar or better accuracy. You can read more [here](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html).\n",
        "\n",
        "The diagram below shows the steps required for calculating the Filter Banks and MFCCs. As you can see, MFCCs require a few more calculations after the Filter Banks:\n",
        "\n",
        "![features pipeline](https://drive.google.com/uc?export=view&id=15T0MgXcj9wA_ZQ_i4tCLukUCH-7T2FnB)\n",
        "\n",
        "https://www.youtube.com/watch?v=QTw-6GU5Mjs&t=319s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3LTCfVc8qYMq"
      },
      "source": [
        "Let's also add a helper function that'll allow us to map the spectrograms to their target text! Specifically, this function will Tokenize the target text (i.e. split the text by character) and Numericize it (i.e. map each character to an integer). We'll also insert some special boundary characters to help the NN read the string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EcJ1gzN7elI"
      },
      "source": [
        "# This tokenization code was borrowed/modified from https://github.com/30stomercury/Automatic-Speech-Recognition/blob/master/utils/tokenizer.py\n",
        "import string\n",
        "def char2id(special_tokens):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        special_tokens: special charactors, <PAD>, <SOS>, <EOS>, <SPACE>\n",
        "    Returns:\n",
        "        char2id: dict, from character to index.\n",
        "        id2char: dict, from index to character.\n",
        "    \"\"\"\n",
        "    alphas = list(string.ascii_uppercase[:26])\n",
        "    tokens = special_tokens + alphas\n",
        "    token_to_id = {}\n",
        "    id_to_token = {}\n",
        "    for i, c in enumerate(tokens):\n",
        "        token_to_id[c] = i\n",
        "        id_to_token[i] = c\n",
        "    return token_to_id, id_to_token\n",
        "\n",
        "SPECIAL_TOKENS = ['<PAD>', '<SOS>', '<EOS>', '<SPACE>']\n",
        "_char2id, _id2char = char2id(SPECIAL_TOKENS)\n",
        "\n",
        "def tokenize(uppercase_sentence):\n",
        "  \"\"\" Tokenize & numericize (i.e. assign integers to each character) the given sentence. \"\"\"\n",
        "  tokens = [_char2id[char] if char != ' ' else _char2id['<SPACE>'] for char in list(uppercase_sentence)]\n",
        "  tokens += [_char2id['<EOS>']]\n",
        "  return tokens"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GH7eT9dVqYMq"
      },
      "source": [
        "\n",
        "def tokenize_sentences(sentences):\n",
        "  \"\"\" Iterate through a list of sentences and tokenize them.\n",
        "  Returns: ( an array of tokens, their lengths )\"\"\"\n",
        "  tokens = []\n",
        "  lengths = []\n",
        "  for sentence in sentences:\n",
        "      # sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "      sentence_converted = tokenize(sentence)\n",
        "      tokens.append(sentence_converted)\n",
        "      lengths.append(len(sentence_converted))\n",
        "  return np.array(tokens), np.array(lengths).astype(np.int32)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYktTn5z_W5V",
        "outputId": "c82b5831-b37f-436a-85a9-6bbc560a789f"
      },
      "source": [
        "# Show examples\n",
        "print(\"Numericization dict: \"+str(_char2id))\n",
        "print(\"'TEST' tokenized is \"+str(tokenize('TEST'))+\" where the 5th token is <EOS>\") "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numericization dict: {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<SPACE>': 3, 'A': 4, 'B': 5, 'C': 6, 'D': 7, 'E': 8, 'F': 9, 'G': 10, 'H': 11, 'I': 12, 'J': 13, 'K': 14, 'L': 15, 'M': 16, 'N': 17, 'O': 18, 'P': 19, 'Q': 20, 'R': 21, 'S': 22, 'T': 23, 'U': 24, 'V': 25, 'W': 26, 'X': 27, 'Y': 28, 'Z': 29}\n",
            "'TEST' tokenized is [23, 8, 22, 23, 2] where the 5th token is <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xOsoUXH-qYMr"
      },
      "source": [
        "Now we can extract features and target text from the whole LibriSpeech dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "--XbcV5HqYMs"
      },
      "source": [
        "# The following code borrowed/modified from https://github.com/30stomercury/Automatic-Speech-Recognition/blob/master/preprocess.py\n",
        "\n",
        "from glob import glob\n",
        "import joblib\n",
        "import logging\n",
        "\n",
        "def get_texts_and_audio_paths(root_dataset_path):\n",
        "  \"\"\" Iterate through folders in a directory.\n",
        "  Return: Target texts, paths of correspending audio recordings \n",
        "  \"\"\"\n",
        "  folders = glob(root_dataset_path+\"/**/**\")\n",
        "  texts = []\n",
        "  audio_path = []\n",
        "  for path in folders:\n",
        "    text_path = glob(path+\"/*txt\")[0]\n",
        "    f = open(text_path)\n",
        "    for line in f.readlines():\n",
        "        line_ = line.split(\" \")\n",
        "        audio_path.append(path+\"/\"+line_[0]+\".flac\")\n",
        "        texts.append(line[len(line_[0])+1:-1].replace(\"'\",\"\"))\n",
        "  return texts, audio_path\n",
        "\n",
        "# When number of audios in a set (usually training set) > threshold, divide set into several parts to avoid memory error.\n",
        "_SAMPLE_THRESHOLD = 30000\n",
        "\n",
        "def extract_inputs(datasets, datasets_dir, feat_dir):\n",
        "  \"\"\" Iterate through LibriSpeech dataset and save features and texts to \n",
        "      binaries under feat_dir. \n",
        "  \"\"\"\n",
        "  def process_libri_feats(audio_path, cat, k):\n",
        "      \"\"\"When number of feats > threshold, divide feature\n",
        "          into several parts to avoid memory error.\n",
        "      \"\"\"\n",
        "      if len(audio_path) > _SAMPLE_THRESHOLD:\n",
        "          featlen = []\n",
        "          n = len(audio_path) // k + 1\n",
        "          logging.info(\"Process {} audios...\".format(cat))\n",
        "          for i in range(k):\n",
        "              feats, featlen_ = files_to_log_mel_spec(audio_path[i*n:(i+1)*n])\n",
        "              featlen += featlen_\n",
        "              # save\n",
        "              joblib.dump(feats, feat_dir+\"/{}-feats-{}.pkl\".format(cat, i))\n",
        "              feats = []\n",
        "      else:\n",
        "          feats, featlen = files_to_log_mel_spec(audio_path, sample_rate, window_size, window_step, n_filters)\n",
        "          joblib.dump(feats, feat_dir+\"/{}-feats.pkl\".format(cat))\n",
        "      np.save(feat_dir+\"/{}-featlen.npy\".format(cat), featlen)\n",
        "\n",
        "  for dataset_name in datasets:\n",
        "      to_cat = dataset_name\n",
        "      libri_path = datasets_dir + dataset_name + '/' + 'LibriSpeech' + '/' + dataset_name\n",
        "      print(libri_path)\n",
        "      target_texts, audio_paths = get_texts_and_audio_paths(libri_path)\n",
        "\n",
        "      # Tokenize sentence\n",
        "      tokens, token_lengths = tokenize_sentences(target_texts)\n",
        "\n",
        "      # Save tokens and their lengths to files\n",
        "      np.save(feat_dir+\"/{}-{}s.npy\".format(to_cat,'char'), tokens)\n",
        "      np.save(feat_dir+\"/{}-{}len.npy\".format(to_cat,'char'), token_lengths)\n",
        "\n",
        "      # Extract and download features (in our case, the Log Mel Spectrograms)\n",
        "      process_libri_feats(audio_paths, to_cat, len(audio_paths)//_SAMPLE_THRESHOLD)\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "DtgqjcI75ju8",
        "outputId": "1e18ffea-722f-4510-93cc-70c5b3db4b67"
      },
      "source": [
        "\n",
        "FEATURES_DIR = './features'\n",
        "!mkdir -p {FEATURES_DIR}\n",
        "extract_inputs(DATASETS, DATASET_DIR, FEATURES_DIR)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./data/dev-clean/LibriSpeech/dev-clean\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-7064a88f1eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFEATURES_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./features'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -p {FEATURES_DIR}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mextract_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURES_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-112-752dec9d552f>\u001b[0m in \u001b[0;36mextract_inputs\u001b[0;34m(datasets, datasets_dir, feat_dir)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# Extract and download features (in our case, the Log Mel Spectrograms)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mprocess_libri_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0m_SAMPLE_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-112-752dec9d552f>\u001b[0m in \u001b[0;36mprocess_libri_feats\u001b[0;34m(audio_path, cat, k)\u001b[0m\n\u001b[1;32m     43\u001b[0m               \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m           \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles_to_log_mel_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m           \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/{}-feats.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/{}-featlen.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-137-4863f8db5551>\u001b[0m in \u001b[0;36mfiles_to_log_mel_spec\u001b[0;34m(filepaths, sample_rate, window_size, window_step, n_filters)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# print(log_mels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# normalize it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mstddevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# print(log_mels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2446\u001b[0m       gen_math_ops.mean(\n\u001b[1;32m   2447\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2448\u001b[0;31m           name=name))\n\u001b[0m\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   5940\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5941\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 5942\u001b[0;31m         _ctx, \"Mean\", name, input, axis, \"keep_dims\", keep_dims)\n\u001b[0m\u001b[1;32m   5943\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5944\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxn4GcKEWsJw",
        "outputId": "f5e2c59e-5dae-46da-8b30-a09d579c4694"
      },
      "source": [
        "!pip install tensorflow_io\n",
        "import tensorflow_io as tfio"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tensorflow<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.18.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.18.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.19.5)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.6.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.34.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (57.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.30.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHi0rGANZzNG",
        "outputId": "636ccad0-954c-4977-b2d8-5fcfd6b8a7b6"
      },
      "source": [
        "!wget -O /content/data/test.wav https://file-examples-com.github.io/uploads/2017/11/file_example_WAV_1MG.wav"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-06 22:18:05--  https://file-examples-com.github.io/uploads/2017/11/file_example_WAV_1MG.wav\n",
            "Resolving file-examples-com.github.io (file-examples-com.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to file-examples-com.github.io (file-examples-com.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073218 (1.0M) [audio/wav]\n",
            "Saving to: ‘/content/data/test.wav’\n",
            "\n",
            "/content/data/test. 100%[===================>]   1.02M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-06-06 22:18:05 (17.0 MB/s) - ‘/content/data/test.wav’ saved [1073218/1073218]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "aDXNC7m5UDk-",
        "outputId": "581e5b32-3334-476c-e8f3-8c208b373c12"
      },
      "source": [
        "\n",
        "def path_to_audio(path, sample_rate):\n",
        "    # spectrogram using stft\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = librosa.load(path, sample_rate)\n",
        "    audio = tf.convert_to_tensor(audio)\n",
        "    # audio, _ = tf.audio.decode_wav(audio, 1)\n",
        "    # audio = tf.squeeze(audio, axis=-1)\n",
        "    # return audio\n",
        "    stfts = tf.signal.stft(audio, frame_length=window_size_frames, frame_step=window_step_frames, fft_length=1024)\n",
        "    spectrograms = tf.abs(stfts)\n",
        "\n",
        "    # Warp the linear scale spectrograms into the mel-scale.\n",
        "    num_spectrogram_bins = stfts.shape[-1]\n",
        "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
        "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "      num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
        "      upper_edge_hertz)\n",
        "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n",
        "      linear_to_mel_weight_matrix.shape[-1:]))\n",
        "\n",
        "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
        "    lmspec = tf.math.log(mel_spectrograms + 1e-6)\n",
        "\n",
        "    # normalisation\n",
        "    means = tf.math.reduce_mean(lmspec, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(lmspec, 1, keepdims=True)\n",
        "    lmspec = (lmspec - means) / stddevs\n",
        "    audio_len = tf.shape(lmspec)[0]\n",
        "    # padding to 10 seconds\n",
        "    pad_len = 2754\n",
        "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
        "    lmspec = tf.pad(lmspec, paddings, \"CONSTANT\")[:pad_len, :]\n",
        "    return lmspec\n",
        "\n",
        "path_to_audio('./data/dev-clean/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac')"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-204-2396cbb2092c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlmspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mpath_to_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/dev-clean/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: path_to_audio() missing 1 required positional argument: 'sample_rate'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8WxuorhlqYMu"
      },
      "source": [
        "Even though we have the spectrograms of audio recordings and their respective text translations, the Neural Network will not be able to read them in this format. We'll need to define the following variables so that both the encoder and decoder can run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FnI61G76qYMv"
      },
      "source": [
        "num_encoder_tokens = None  #\n",
        "num_decoder_tokens = None\n",
        "encoder_input_data = None\n",
        "decoder_input_data = None\n",
        "max_encoder_seq_length = None\n",
        "max_decoder_seq_length = None\n",
        "\n",
        "decoder_target_data = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YkGnjZhAqYMv"
      },
      "source": [
        "\n",
        "    # target_token_index = dict([(char, i) for i, char in enumerate(total_found_chars)])\n",
        "\n",
        "def get_decoder_data():  # populate 2 arrays, offset by 1\n",
        "    decoder_input_data = np.zeros(\n",
        "        (num_output, max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (num_output, max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    for char in found_chars:\n",
        "        idx =target_token_index[char]\n",
        "        array[idx] = 1.0\n",
        "\n",
        "        # How do we do input/output arrays of diffff length?\n",
        "        # https://keras.io/examples/nlp/lstm_seq2seq/\n",
        "        # https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/lstm_seq2seq.ipynb#scrollTo=Tp6YF0oHXgay\n",
        "        # https://medium.com/deep-learning-with-keras/seq2seq-part-e-encoder-decoder-for-variable-input-output-size-with-teacher-forcing-92c476dd9b0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "alnoWfruqYMw"
      },
      "source": [
        "# add function to AudioHandler\n",
        "log_specgrams, \\\n",
        "target_text_chars, \\\n",
        "total_found_chars, \\\n",
        "num_target_text_tokens = extract_features_and_text_targets('./data_test/LibriSpeech/dev-clean/')\n",
        "# https://www.kdnuggets.com/2017/12/audio-classifier-deep-neural-networks.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "UPmVRqt2qYMx"
      },
      "source": [
        " and Numericize them (i.e. assign integers to each token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RLwodtBUqYMx",
        "outputId": "fe794a81-1c7c-4a37-9496-3bdb59035056"
      },
      "source": [
        "target_token_index = None  # A dict mapping an integer to each seen character\n",
        "encoder_input_data = None\n",
        "decoder_input_data = None  # An array of dicts mapping characters to whether 1 if seen, 0 if not seen\n",
        "decoder_target_data = None  # An array of dicts mapping characters to whether 1 if seen, 0 if not seen\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1272-135031-0002\n",
            "1272-135031-0003\n",
            "1272-135031-0018\n",
            "1272-135031-0019\n",
            "1272-135031-0011\n",
            "1272-135031-0022\n",
            "1272-135031-0020\n",
            "1272-135031-0007\n",
            "1272-135031-0006\n",
            "1272-135031-0009\n",
            "1272-135031-0014\n",
            "1272-135031-0010\n",
            "1272-135031-0013\n",
            "1272-135031-0017\n",
            "1272-135031-0021\n",
            "1272-135031-0000\n",
            "1272-135031-0005\n",
            "1272-135031-0023\n",
            "1272-135031-0016\n",
            "1272-135031-0008\n",
            "1272-135031-0012\n",
            "1272-135031-0001\n",
            "1272-135031-0015\n",
            "1272-135031-0004\n",
            "1272-135031-0024\n",
            "1272-128104-0005\n",
            "1272-128104-0008\n",
            "1272-128104-0014\n",
            "1272-128104-0011\n",
            "1272-128104-0001\n",
            "1272-128104-0013\n",
            "1272-128104-0012\n",
            "1272-128104-0004\n",
            "1272-128104-0007\n",
            "1272-128104-0002\n",
            "1272-128104-0003\n",
            "1272-128104-0000\n",
            "1272-128104-0009\n",
            "1272-128104-0006\n",
            "1272-128104-0010\n",
            "1272-141231-0011\n",
            "1272-141231-0018\n",
            "1272-141231-0030\n",
            "1272-141231-0013\n",
            "1272-141231-0012\n",
            "1272-141231-0010\n",
            "1272-141231-0019\n",
            "1272-141231-0022\n",
            "1272-141231-0006\n",
            "1272-141231-0032\n",
            "1272-141231-0001\n",
            "1272-141231-0025\n",
            "1272-141231-0027\n",
            "1272-141231-0026\n",
            "1272-141231-0015\n",
            "1272-141231-0003\n",
            "1272-141231-0024\n",
            "1272-141231-0028\n",
            "1272-141231-0017\n",
            "1272-141231-0020\n",
            "1272-141231-0002\n",
            "1272-141231-0031\n",
            "1272-141231-0004\n",
            "1272-141231-0016\n",
            "1272-141231-0007\n",
            "1272-141231-0008\n",
            "1272-141231-0023\n",
            "1272-141231-0009\n",
            "1272-141231-0000\n",
            "1272-141231-0014\n",
            "1272-141231-0029\n",
            "1272-141231-0005\n",
            "1272-141231-0021\n",
            "1673-143396-0010\n",
            "1673-143396-0017\n",
            "1673-143396-0013\n",
            "1673-143396-0000\n",
            "1673-143396-0007\n",
            "1673-143396-0012\n",
            "1673-143396-0011\n",
            "1673-143396-0003\n",
            "1673-143396-0005\n",
            "1673-143396-0001\n",
            "1673-143396-0002\n",
            "1673-143396-0006\n",
            "1673-143396-0015\n",
            "1673-143396-0019\n",
            "1673-143396-0016\n",
            "1673-143396-0018\n",
            "1673-143396-0004\n",
            "1673-143396-0008\n",
            "1673-143396-0014\n",
            "1673-143396-0009\n",
            "1673-143396-0020\n",
            "1673-143397-0003\n",
            "1673-143397-0007\n",
            "1673-143397-0005\n",
            "1673-143397-0000\n",
            "1673-143397-0020\n",
            "1673-143397-0004\n",
            "1673-143397-0002\n",
            "1673-143397-0013\n",
            "1673-143397-0016\n",
            "1673-143397-0009\n",
            "1673-143397-0014\n",
            "1673-143397-0001\n",
            "1673-143397-0008\n",
            "1673-143397-0015\n",
            "1673-143397-0010\n",
            "1673-143397-0018\n",
            "1673-143397-0012\n",
            "1673-143397-0006\n",
            "1673-143397-0017\n",
            "1673-143397-0011\n",
            "1673-143397-0019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hJR9yKQZqYMy"
      },
      "source": [
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "data_path = \"./data/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac\"\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9Lq0x61NqYMz",
        "outputId": "10029c60-8e29-42b5-e447-25fb09ca264a"
      },
      "source": [
        "import keras\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<module 'keras.datasets.boston_housing' from '/home/a/.local/lib/python3.8/site-packages/keras/datasets/boston_housing.py'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FJD5RiMhqYMz"
      },
      "source": [
        "\n",
        "Next: build encoder-decoder architecture, based on https://github.com/tensorflow/nmt\n",
        "\n",
        "Next: plug into RNN, run it on Google Colab. Save it\n",
        "\n",
        "Next: let user input voice data, and run it through the model\n",
        "\n",
        "More soucres\n",
        "https://towardsdatascience.com/recognizing-speech-commands-using-recurrent-neural-networks-with-attention-c2b2ba17c837\n",
        "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
        "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
        "https://arxiv.org/pdf/1409.0473.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "W3mz2H6iqYMz"
      },
      "source": [
        "\n",
        "Further Optimizations:\n",
        "\n",
        "The Context Vector being fixed-length turns out to be a performance bottleneck; given a long input sentences, the the Encoder may not be able to store all of its output in the Context Vector in one timestep.\n",
        "\n",
        "To solve the bottleneck, we could use the 'attention' mechanism which involves the Decoder selecting from all hidden states provided by the Encoder. For the sake of simplicity we won't be doing this optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDCvyHwywhif"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "pycharm-5af6fa8",
      "language": "python",
      "display_name": "PyCharm (001_game_of_life)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "index.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eMIen1MlqYMi"
      },
      "source": [
        "In this notebook, I'll provide an wide overview on Automatic Speech Architecture, its standard pipelines, and how to train a model for it!\n",
        "\n",
        "### What is Automatic Speech Recognition?\n",
        "Automatic Speech Recognition (ASR) is the translation of spoken speech to text by a computer. This typically only means turning audio into a sequence of characters (disregarding grammar).\n",
        "\n",
        "It has many uses:\n",
        "+ closed captioning\n",
        "+ mobile phone voice assistants\n",
        "+ interface for handicapped individuals\n",
        "+ preserve endangered languages\n",
        "\n",
        "However, ASR is very difficult to solve due to:\n",
        "+ Environment (Is there noise? Other speakers?)\n",
        "+ Style of speech (Is it casual, formal, poetic, etc?)\n",
        "+ Style of speaker (Talks fast? Accents? Gender bias?)\n",
        "\n",
        "Modern ASR models work quite well in ideal, noiseless conditions. However, in noisy conditions they are far from perfect. Below, we can see the typical accuracy for state-of-the-art ASR models in 2020!\n",
        "\n",
        "(Note: To calculate the accuracy of ASR models, we use a 'Word Error Rate.' This determines which percentage of spoken words were incorrectly translated to text by the ASR model. )\n",
        "\n",
        "![word error rate](https://drive.google.com/uc?export=view&id=1iUz9koIqQErBVB0a4db0gSZVdfsfwh9L)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eUVn2wDxqYMj"
      },
      "source": [
        "\n",
        "Architecture:\n",
        "\n",
        "![timeline](https://drive.google.com/uc?export=view&id=1n2ID2ML5x9TFSBJhfdvj5AGI4u-9Mp4m)  \n",
        "[source](https://youtu.be/q67z7PTGRi8)\n",
        "\n",
        "\n",
        "From the 1980s to the 2010s, ASR was predominately performed by Hidden Markov Model (HMM) architecture. HMMs are probabilistic models for linear sequence classification problems. Given text's linear nature, HMMs seem like a natural fit for ASR.\n",
        "However, using HMMs for ASR come with 3 downsides:\n",
        "+ HMMs are built on the Markov Assumption, which assumes that the probability of the next state only depends on the current state, and not any states prior. This doesn't make sense for language processing, which is highly contextual (see Coarticulation)\n",
        "+ HMMs' state transitions probabilities are 'baked in' and thus inflexible to changes in language\n",
        "+ Classic HMM ASR pipelines require hand-tuned probability distributions for accoustic models and language models from linguistic experts\n",
        "+ HMM-based ASR architecture is very complex, requiring 3 models as input. The figure below demonstrates an HMM-centric ASR architecture\n",
        "\n",
        "![an HMM-based architecture](https://drive.google.com/uc?export=view&id=1loF8wbD-6DRO45Ly7lKnVVIcCn83H-B9)  \n",
        "[source](https://youtu.be/q67z7PTGRi8)\n",
        "\n",
        "## Neural Networks\n",
        "These days Neural Network architecture, like Recurrent Neural Networks (RNNs) or RNN-HMM hybrids are favored for ASR because they solve these problems. In essence, Neural Networks take an arbitrary amount of data points and tries to 'fit' a predictive model to those points. The introduction of Neural Networks to ASR immediately led to WER to be improved by ~30%. However, RNN's come at the cost of requiring a vast amount of training data.\n",
        "\n",
        "![an RNN-only architecture](https://drive.google.com/uc?export=view&id=1_9McwMlHIqlPuJGYOkPwB-8h-t13P4pI)  \n",
        "(an RNN-only model that has only 4 steps)  \n",
        "[source](https://youtu.be/q67z7PTGRi8)\n",
        "\n",
        "## Encoder-Decoder\n",
        "ASR is typically performed with an Encoder-Decoder Recurrent Neural Network, which was developed as a way of solving Sequence-to-Sequence prediction problems; essentially this means taking an input sequence and returning an output sequence. These sequences can hold elements of arbitrary length- making this architecture viable (and powerful!) for problems involving inputs and outputs of different lengths. This is precisely why it's used for ASR!\n",
        "\n",
        "Encoder-Decoder models are composed of 3 parts:\n",
        "+ an Encoder (a Neural Network)\n",
        "+ a Context Vector (a vector of a 'a summary of the inputs')\n",
        "+ a Decoder (a Neural Network)\n",
        "\n",
        "The Encoder compresses and summarizes input data into a Context Vector, which is later transformed by the Decoder. The Context Vector must be defined with a fixed-length (usually 256, 512, or 1024).\n",
        "\n",
        "## Accoustic Features as Input for RNN:\n",
        "A classic HMM-based ASR model would require us to input 3 probabilistic models (Accoustic, Pronunciation, and Language). An RNN only *requires* 1 (Accoustic), but a hand-crafted Language model is still commonly used for the purposes of injecting rules and grammar to speech-to-text translations. \n",
        "\n",
        "The Accoustic Model regards how Accoustic Features (from audio) are broken up into phonemes (the building blocks of speech) and other linguistic elements.  We'll extract this data from the [LibriSpeech ASR Dataset](https://www.openslr.org/12).  \n",
        "\n",
        "The Language Model is a probabilistic model that concerns the stringing of words and other language elements together in a sensible way. In state-of-the-art architectures like those at Google, Language Models are integrated into deep learning models. In this notebook we won't be using this, and instead only building an Accoustic Model.\n",
        "\n",
        "For our model to build an Accostic Model need to first extract Accoustic Features from the audio files- we'll be putting these features in a vector and feeding them into the RNN.\n",
        "In order to extract the features from a given file, we need to perform the following steps:\n",
        "```\n",
        "1) Convert the FLAC file into a Waveform (giving us frequency data over the time domain)\n",
        "2) Split the Waveform into Windows using a Windowing Function (like Hamming)\n",
        "3) For each Window:\n",
        "    a) Compute its Fourier Transform\n",
        "    b) Compute its Mel Spectrogram (also known as Mel Filter Bank)\n",
        "    c) Compute the Log of each value in the Mel Spectrogram\n",
        "    d) Append the result of 3c to the Features Vector\n",
        "4) Return Features Vector\n",
        "```\n",
        "\n",
        "But before we do any computations, let's define some configuration constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ENSYoGp4e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063fa5ae-a9e6-439e-82a9-04d5bda6036e"
      },
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from glob import glob\n",
        "import joblib\n",
        "from tqdm import tqdm # for pretty loading bars\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "!pip install tensorflow_io\n",
        "import tensorflow_io as tfio\n",
        "import string\n",
        "!sudo apt install ffmpeg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.18.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.18.0)\n",
            "Requirement already satisfied: tensorflow<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.2)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.34.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.12.4)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.36.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (57.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.7.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AXrfGRTpqYMl"
      },
      "source": [
        "sample_rate = 16000  # The Sample Rate of the LibriSpeech Dataset\n",
        "window_size_frames = int(sample_rate * 0.025)  # The standard window size for ASR models is 25ms\n",
        "window_step_frames = int(sample_rate * 0.010) # The standard window step for ASR models is 10ms\n",
        "n_filters = 40\n",
        "\n",
        "DATASETS = [\n",
        "  \"dev-clean\"\n",
        "  # \"train-clean-100\"\n",
        "  # \"train-clean-360\"  # this exceeds Colab's Disk limit\n",
        "  # \"train-clean-500\"  # this exceeds Colab's Disk limit\n",
        "]\n",
        "DATASET_DIR = './data/'\n",
        "FEATURES_DIR = './features'\n",
        "\n",
        "# SPECIAL_TOKENS = ['<PAD>', '<SPC>', '<SOS>', '<EOS>']  # Tags for text formatting\n",
        "SPECIAL_TOKENS = [\"-\", \" \", \"<\", \">\", \" \", \"'\", '\"']  # Tags for text formatting\n",
        "VOCAB =  SPECIAL_TOKENS + list(string.ascii_uppercase[:26])\n",
        "\n",
        "MAX_TEXT_LEN = 700 # >the length of the largest sentence. Used for padding"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDeN6xz5wvDY"
      },
      "source": [
        "And of course, download the dataset(s):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5xKocQIsvmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192c715a-63f5-48c8-b652-71db49da8b84"
      },
      "source": [
        "\n",
        "\n",
        "def download_datasets(datasets): \n",
        "  for dataset in datasets:\n",
        "    dest = DATASET_DIR + dataset + '.tar.gz'\n",
        "    !mkdir -p {DATASET_DIR}\n",
        "    !wget -O {dest} https://www.openslr.org/resources/12/{dataset}.tar.gz\n",
        "\n",
        "def extract_datasets(datasets):\n",
        "  for dataset in datasets:\n",
        "    src = DATASET_DIR + dataset + \".tar.gz\"\n",
        "    dest = DATASET_DIR + dataset + '/'\n",
        "    !mkdir -p {DATASET_DIR + dataset}\n",
        "    !tar -xf {src} -C {dest}\n",
        "    print(\"Finished extraction\")\n",
        "\n",
        "def convert_datasets_to_wav(datasets):\n",
        "  \"\"\" Iterate through datasets and convert .flac files to .wav. Needed for \n",
        "  Tensors during Log Mel Spectrogram calculation\n",
        "  \"\"\"\n",
        "  dataset_dirs = [DATASET_DIR + dataset + '/LibriSpeech/' + dataset for dataset in datasets]\n",
        "  for dataset_dir in dataset_dirs:\n",
        "    folders = glob(dataset_dir+\"/**/**\")\n",
        "    print(folders)\n",
        "    texts = []\n",
        "    audio_path = []\n",
        "    for path in folders:\n",
        "      flac_paths = glob(path+\"/*flac\")\n",
        "      for flac_path in flac_paths:\n",
        "        target = flac_path[:-5]+\".wav\"\n",
        "        os.system(f'ffmpeg -i {flac_path} {target}')\n",
        "\n",
        "download_datasets(DATASETS)\n",
        "extract_datasets(DATASETS)\n",
        "convert_datasets_to_wav(DATASETS)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-09 02:31:53--  https://www.openslr.org/resources/12/dev-clean.tar.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 337926286 (322M) [application/x-gzip]\n",
            "Saving to: ‘./data/dev-clean.tar.gz’\n",
            "\n",
            "./data/dev-clean.ta 100%[===================>] 322.27M  23.5MB/s    in 15s     \n",
            "\n",
            "2021-06-09 02:32:08 (21.9 MB/s) - ‘./data/dev-clean.tar.gz’ saved [337926286/337926286]\n",
            "\n",
            "Finished extraction\n",
            "['./data/dev-clean/LibriSpeech/dev-clean/6241/61946', './data/dev-clean/LibriSpeech/dev-clean/6241/61943', './data/dev-clean/LibriSpeech/dev-clean/6241/66616', './data/dev-clean/LibriSpeech/dev-clean/3853/163249', './data/dev-clean/LibriSpeech/dev-clean/251/118436', './data/dev-clean/LibriSpeech/dev-clean/251/137823', './data/dev-clean/LibriSpeech/dev-clean/251/136532', './data/dev-clean/LibriSpeech/dev-clean/3752/4944', './data/dev-clean/LibriSpeech/dev-clean/3752/4943', './data/dev-clean/LibriSpeech/dev-clean/1462/170142', './data/dev-clean/LibriSpeech/dev-clean/1462/170145', './data/dev-clean/LibriSpeech/dev-clean/1462/170138', './data/dev-clean/LibriSpeech/dev-clean/5694/64029', './data/dev-clean/LibriSpeech/dev-clean/5694/64025', './data/dev-clean/LibriSpeech/dev-clean/5694/64038', './data/dev-clean/LibriSpeech/dev-clean/2412/153947', './data/dev-clean/LibriSpeech/dev-clean/2412/153948', './data/dev-clean/LibriSpeech/dev-clean/2412/153954', './data/dev-clean/LibriSpeech/dev-clean/2902/9006', './data/dev-clean/LibriSpeech/dev-clean/2902/9008', './data/dev-clean/LibriSpeech/dev-clean/777/126732', './data/dev-clean/LibriSpeech/dev-clean/3576/138058', './data/dev-clean/LibriSpeech/dev-clean/174/168635', './data/dev-clean/LibriSpeech/dev-clean/174/84280', './data/dev-clean/LibriSpeech/dev-clean/174/50561', './data/dev-clean/LibriSpeech/dev-clean/7976/110523', './data/dev-clean/LibriSpeech/dev-clean/7976/110124', './data/dev-clean/LibriSpeech/dev-clean/7976/105575', './data/dev-clean/LibriSpeech/dev-clean/6295/64301', './data/dev-clean/LibriSpeech/dev-clean/6295/244435', './data/dev-clean/LibriSpeech/dev-clean/1919/142785', './data/dev-clean/LibriSpeech/dev-clean/6313/76958', './data/dev-clean/LibriSpeech/dev-clean/6313/66129', './data/dev-clean/LibriSpeech/dev-clean/6313/66125', './data/dev-clean/LibriSpeech/dev-clean/2078/142845', './data/dev-clean/LibriSpeech/dev-clean/3000/15664', './data/dev-clean/LibriSpeech/dev-clean/2803/154328', './data/dev-clean/LibriSpeech/dev-clean/2803/154320', './data/dev-clean/LibriSpeech/dev-clean/2803/161169', './data/dev-clean/LibriSpeech/dev-clean/8842/302201', './data/dev-clean/LibriSpeech/dev-clean/8842/304647', './data/dev-clean/LibriSpeech/dev-clean/8842/302196', './data/dev-clean/LibriSpeech/dev-clean/8842/302203', './data/dev-clean/LibriSpeech/dev-clean/7850/111771', './data/dev-clean/LibriSpeech/dev-clean/7850/73752', './data/dev-clean/LibriSpeech/dev-clean/7850/286674', './data/dev-clean/LibriSpeech/dev-clean/7850/281318', './data/dev-clean/LibriSpeech/dev-clean/652/130737', './data/dev-clean/LibriSpeech/dev-clean/652/130726', './data/dev-clean/LibriSpeech/dev-clean/652/129742', './data/dev-clean/LibriSpeech/dev-clean/6345/93302', './data/dev-clean/LibriSpeech/dev-clean/6345/93306', './data/dev-clean/LibriSpeech/dev-clean/6345/64257', './data/dev-clean/LibriSpeech/dev-clean/6319/275224', './data/dev-clean/LibriSpeech/dev-clean/6319/57405', './data/dev-clean/LibriSpeech/dev-clean/6319/64726', './data/dev-clean/LibriSpeech/dev-clean/8297/275154', './data/dev-clean/LibriSpeech/dev-clean/8297/275156', './data/dev-clean/LibriSpeech/dev-clean/8297/275155', './data/dev-clean/LibriSpeech/dev-clean/84/121550', './data/dev-clean/LibriSpeech/dev-clean/84/121123', './data/dev-clean/LibriSpeech/dev-clean/2035/147961', './data/dev-clean/LibriSpeech/dev-clean/2035/152373', './data/dev-clean/LibriSpeech/dev-clean/2035/147960', './data/dev-clean/LibriSpeech/dev-clean/5895/34615', './data/dev-clean/LibriSpeech/dev-clean/5895/34622', './data/dev-clean/LibriSpeech/dev-clean/5895/34629', './data/dev-clean/LibriSpeech/dev-clean/1988/147956', './data/dev-clean/LibriSpeech/dev-clean/1988/24833', './data/dev-clean/LibriSpeech/dev-clean/1988/148538', './data/dev-clean/LibriSpeech/dev-clean/2277/149874', './data/dev-clean/LibriSpeech/dev-clean/2277/149897', './data/dev-clean/LibriSpeech/dev-clean/2277/149896', './data/dev-clean/LibriSpeech/dev-clean/3170/137482', './data/dev-clean/LibriSpeech/dev-clean/2428/83699', './data/dev-clean/LibriSpeech/dev-clean/2428/83705', './data/dev-clean/LibriSpeech/dev-clean/1993/147964', './data/dev-clean/LibriSpeech/dev-clean/1993/147965', './data/dev-clean/LibriSpeech/dev-clean/1993/147966', './data/dev-clean/LibriSpeech/dev-clean/1993/147149', './data/dev-clean/LibriSpeech/dev-clean/3536/8226', './data/dev-clean/LibriSpeech/dev-clean/3536/23268', './data/dev-clean/LibriSpeech/dev-clean/1272/135031', './data/dev-clean/LibriSpeech/dev-clean/1272/128104', './data/dev-clean/LibriSpeech/dev-clean/1272/141231', './data/dev-clean/LibriSpeech/dev-clean/3081/166546', './data/dev-clean/LibriSpeech/dev-clean/5338/24640', './data/dev-clean/LibriSpeech/dev-clean/5338/284437', './data/dev-clean/LibriSpeech/dev-clean/5338/24615', './data/dev-clean/LibriSpeech/dev-clean/5536/43359', './data/dev-clean/LibriSpeech/dev-clean/5536/43358', './data/dev-clean/LibriSpeech/dev-clean/5536/43363', './data/dev-clean/LibriSpeech/dev-clean/1673/143396', './data/dev-clean/LibriSpeech/dev-clean/1673/143397', './data/dev-clean/LibriSpeech/dev-clean/2086/149214', './data/dev-clean/LibriSpeech/dev-clean/2086/149220', './data/dev-clean/LibriSpeech/dev-clean/422/122949']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "cShnG7OEqYMm"
      },
      "source": [
        "Now let's get the Log Mel Spectrograms!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Uq6BRWyO0jjG"
      },
      "source": [
        "\n",
        "\n",
        "# def files_to_log_mel_spec(paths, sample_rate=16000, window_size_frames=400, window_step_frames=160, num_mel_bins=40):\n",
        "def files_to_log_mel_spec(path):\n",
        "  \"\"\" Returns \n",
        "  ( Tensor of Log Mel Spectrograms of the audio files in paths, \n",
        "    numpy array of the Lengths of the Spectrograms ) \n",
        "  \"\"\"\n",
        "  audio = tf.io.read_file(path)\n",
        "  audio, _ = tf.audio.decode_wav(audio, 1)\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  # Calculate Spectrogram\n",
        "  stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
        "  spectrograms = tf.abs(stfts)\n",
        "  # Warp the linear scale spectrograms into the Mel Scale.\n",
        "  num_spectrogram_bins = stfts.shape[-1]\n",
        "  lower_edge_hertz, upper_edge_hertz = 80.0, 7600.0\n",
        "  linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "    n_filters, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
        "    upper_edge_hertz)\n",
        "  mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "  mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n",
        "    linear_to_mel_weight_matrix.shape[-1:]))\n",
        "  # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
        "  lmspec = tf.math.log(mel_spectrograms + 1e-6)\n",
        "\n",
        "  # Normalize Log Mel Spectrogram\n",
        "  means = tf.math.reduce_mean(lmspec, 1, keepdims=True)\n",
        "  stddevs = tf.math.reduce_std(lmspec, 1, keepdims=True)\n",
        "  lmspec = (lmspec - means) / stddevs\n",
        "\n",
        "  # Pad to make consistent (required for Neural Network)\n",
        "  pad_len = 4500 # Longest in dev-clean is 3263\n",
        "  paddings = tf.constant([[0, pad_len], [0, 0]])\n",
        "  lmspec = tf.pad(lmspec, paddings, \"CONSTANT\")[:pad_len, :]\n",
        "  return lmspec\n",
        "  \n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pKKC96A4omz"
      },
      "source": [
        "# # Example\n",
        "# audio = files_to_log_mel_spec(['./data/dev-clean/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac'], sample_rate, window_size_frames, window_step_frames, n_filters)\n",
        "# audio = files_to_log_mel_spec('./data/dev-clean/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9ytf-ldeqYMo"
      },
      "source": [
        "Sometimes you may see MFCCs used as the ASR features instead of Mel Filter Banks. MFCCs are values that loosely represents the brain's capacity to filter out certain signals. MFCCs were popular with HMM-based models for helping model based on human-perceptible information, but newer architecture can usually use Filter Banks and get similar or better accuracy. You can read more [here](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html).\n",
        "\n",
        "The diagram below shows the steps required for calculating the Filter Banks and MFCCs. As you can see, MFCCs require a few more calculations after the Filter Banks:\n",
        "\n",
        "![features pipeline](https://drive.google.com/uc?export=view&id=15T0MgXcj9wA_ZQ_i4tCLukUCH-7T2FnB)\n",
        "\n",
        "[source](https://www.youtube.com/watch?v=QTw-6GU5Mjs&t=319s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3LTCfVc8qYMq"
      },
      "source": [
        "Let's also add a helper function that'll allow us to map the spectrograms to their target text! Specifically, this function will Tokenize the target text (i.e. split the text by character) and Numericize it (i.e. map each character to an integer). We'll also insert some special boundary characters to help the NN read the string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EcJ1gzN7elI"
      },
      "source": [
        "# This tokenization code was borrowed/modified from https://github.com/30stomercury/Automatic-Speech-Recognition/blob/master/utils/tokenizer.py\n",
        "import string\n",
        "def char2id(vocab):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        special_tokens: special charactors, #, <, >, _\n",
        "    Returns:\n",
        "        char2id: dict, from character to index.\n",
        "        id2char: dict, from index to character.\n",
        "    \"\"\"\n",
        "    tokens = vocab\n",
        "    token_to_id = {}\n",
        "    id_to_token = {}\n",
        "    for i, c in enumerate(tokens):\n",
        "        token_to_id[c] = i\n",
        "        id_to_token[i] = c\n",
        "    return token_to_id, id_to_token\n",
        "\n",
        "_char2id, _id2char = char2id(VOCAB)  # Lookup dictionaries\n",
        "\n",
        "def tokenize(uppercase_sentence):\n",
        "  \"\"\" Returns a Tokenization of the given sentence and its length \"\"\"\n",
        "  tokens = [_char2id[SPECIAL_TOKENS[2]]] # Put Start-of-Sentence character\n",
        "  tokens += [_char2id[char] if char != ' ' else _char2id[SPECIAL_TOKENS[4]] for char in list(uppercase_sentence)]\n",
        "  tokens += [_char2id[SPECIAL_TOKENS[3]]] # Put End-of-Sentence character\n",
        "  return np.array(tokens, dtype=np.int32), len(tokens)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GH7eT9dVqYMq"
      },
      "source": [
        "\n",
        "def tokenize_sentences(sentences):\n",
        "  \"\"\" Iterate through a list of sentences and tokenize them.\n",
        "  Returns: ( an array of tokens, their lengths )\"\"\"\n",
        "  tokens = []\n",
        "  lengths = []\n",
        "  n_sentences = len(sentences)\n",
        "  print(\"Tokenizing \"+str(n_sentences)+\" sentences\")\n",
        "  for sentence in sentences:\n",
        "    # sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    sentence_converted, length = tokenize(sentence)\n",
        "    # Pad Tokenization\n",
        "    to_pad = MAX_TEXT_LEN - length # max in dev_clean is 513\n",
        "    sentence_converted = np.concatenate((sentence_converted, [_char2id[SPECIAL_TOKENS[0]]] * to_pad))\n",
        "    tokens.append(sentence_converted)\n",
        "    lengths.append(length)\n",
        "  return np.array(tokens, dtype=np.int32), np.array(lengths).astype(np.int32)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYktTn5z_W5V",
        "outputId": "b5b2f06a-3542-47d9-f096-b7a8a5e04906"
      },
      "source": [
        "# Show examples\n",
        "print(\"Numericization dict: \"+str(_char2id))\n",
        "print(\"'TEST' tokenized is \"+str(tokenize('TEST')[0])+\" where the 1st token is < and the last token is >\") "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numericization dict: {'-': 0, ' ': 4, '<': 2, '>': 3, \"'\": 5, '\"': 6, 'A': 7, 'B': 8, 'C': 9, 'D': 10, 'E': 11, 'F': 12, 'G': 13, 'H': 14, 'I': 15, 'J': 16, 'K': 17, 'L': 18, 'M': 19, 'N': 20, 'O': 21, 'P': 22, 'Q': 23, 'R': 24, 'S': 25, 'T': 26, 'U': 27, 'V': 28, 'W': 29, 'X': 30, 'Y': 31, 'Z': 32}\n",
            "'TEST' tokenized is [ 2 26 11 25 26  3] where the 1st token is < and the last token is >\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xOsoUXH-qYMr"
      },
      "source": [
        "Now we can extract features and target text from the whole LibriSpeech dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "--XbcV5HqYMs"
      },
      "source": [
        "\n",
        "def extract_tokens_and_paths(datasets, datasets_dir, feat_dir):\n",
        "  \"\"\" Iterate through LibriSpeech dataset and save features and texts to \n",
        "      binaries under feat_dir. \n",
        "  \"\"\"\n",
        "\n",
        "  def get_texts_and_audio_paths(root_dataset_path):\n",
        "    \"\"\" Iterate through folders in a directory.\n",
        "    Return: Target texts, paths of correspending audio recordings \n",
        "    \"\"\"\n",
        "    folders = glob(root_dataset_path+\"/**/**\")\n",
        "    texts = []\n",
        "    audio_path = []\n",
        "    for path in folders:\n",
        "      text_path = glob(path+\"/*txt\")[0]\n",
        "      f = open(text_path)\n",
        "      for line in f.readlines():\n",
        "          line_ = line.split(\" \")\n",
        "          audio_path.append(path+\"/\"+line_[0]+\".wav\")\n",
        "          texts.append(line[len(line_[0])+1:-1].replace(\"'\",\"\"))\n",
        "    return texts, audio_path\n",
        "    \n",
        "  # Main extraction\n",
        "  audio_paths_all = []\n",
        "  for dataset_name in datasets:\n",
        "    to_cat = dataset_name\n",
        "    libri_path = datasets_dir + dataset_name + '/' + 'LibriSpeech' + '/' + dataset_name\n",
        "    print(\"Extracting from \"+libri_path)\n",
        "    target_texts, audio_paths = get_texts_and_audio_paths(libri_path)\n",
        "    audio_paths_all += audio_paths\n",
        "\n",
        "    # Tokenize sentence\n",
        "    tokens, token_lengths = tokenize_sentences(target_texts)\n",
        "\n",
        "    # Save tokens and their lengths to files\n",
        "    np.save(feat_dir+\"/{}-{}s.npy\".format(to_cat,'char'), tokens)\n",
        "    np.save(feat_dir+\"/{}-{}len.npy\".format(to_cat,'char'), token_lengths)\n",
        "\n",
        "  print(\"Finished extraction\")\n",
        "  return audio_paths_all\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtgqjcI75ju8",
        "outputId": "4c032996-2a10-44b6-967d-b8703c0ebdf3"
      },
      "source": [
        "\n",
        "!mkdir -p {FEATURES_DIR}\n",
        "audio_paths = extract_tokens_and_paths(DATASETS, DATASET_DIR, FEATURES_DIR)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting from ./data/dev-clean/LibriSpeech/dev-clean\n",
            "Tokenizing 2703 sentences\n",
            "Finished extraction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjWggoD3GcNh"
      },
      "source": [
        "\n",
        "def load_extracted_tokens(feature_dir, datasets):\n",
        "  \"\"\" Load the data from files \"\"\"\n",
        "  dataset = datasets[0] # TODO\n",
        "  tokens = np.load(feature_dir+\"/{}-chars.npy\".format(dataset), allow_pickle=True)\n",
        "  token_lengths = np.load(feature_dir+\"/{}-charlen.npy\".format(dataset), allow_pickle=True)\n",
        "  return tokens, token_lengths\n",
        "\n",
        "# Load our saved data\n",
        "tokens, token_lengths = load_extracted_tokens(FEATURES_DIR, DATASETS)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17I-D8UCILms"
      },
      "source": [
        "def create_tensorflow_datasets(features, tokens, batch_size, audio_paths):\n",
        "  \"\"\" Return Tensorflow Dataset objects for features, tokens \"\"\"\n",
        "  features_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "  features_ds = features_ds.map(\n",
        "      files_to_log_mel_spec, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "\n",
        "  tokens_ds = tf.data.Dataset.from_tensor_slices(tokens)\n",
        "\n",
        "  ds = tf.data.Dataset.zip((features_ds, tokens_ds))\n",
        "  ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return ds\n",
        "\n",
        "split = int(len(tokens) * 0.99)\n",
        "# Training dataset\n",
        "train_features = [] #features[:split]  # junk for now\n",
        "train_tokens = tokens[:split]\n",
        "train_audio_paths = audio_paths[:split]\n",
        "train_ds = create_tensorflow_datasets(train_features, train_tokens, 64, train_audio_paths)\n",
        "\n",
        "# Testing dataset\n",
        "test_features = [] #features[split:]\n",
        "test_tokens = tokens[split:]\n",
        "test_audio_paths = audio_paths[split:]\n",
        "test_ds = create_tensorflow_datasets(test_features, test_tokens, 4, test_audio_paths)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e05sHJGIpxa"
      },
      "source": [
        "## Current-day Encoder-Decoder Architectures for ASR\n",
        "\n",
        "### Attention Optimization\n",
        "A weakness to the plain encoder-decoder model is that accuracy of predictions is worse with longer sequences of input. The issue lies in the inability of the model to use distant information to predict words farther in the sentence, which is caused by the Context Vector being fixed in size and unable to store long sentences. Due to how the beginning of a sentence strongly influences the rest of the sentence, an solution to this weakness was needed. In 2014 a [landmark paper](https://arxiv.org/abs/1409.0473) (Bahdanau et. al) introduced the concept of an \"attention mechanism.\" Intuitively, it allows the model to pay *attention* to any part of the input when making predictions. Structurally we implement Attention as an vector of weights summing to 1 that determine how much attention we pay to that input in the sequence.\n",
        "\n",
        "### Listen, Attend, and Spell\n",
        "The ['Listen, Attend, and Spell'](https://arxiv.org/abs/1508.01211) (LAS) model was proposed by in 2015 for ASR architecture in order to simplify Hybrid architecture (such as DNN-HMMs or CTC-HMMs) and avoid the aforementioned independence assumptions. It was one of the first models for ASR to utilize the attention mechanism to great success.\n",
        "\n",
        "### Transformer\n",
        "Transformers are the model of choice for modern NLP problems (used in projects like GPT and BERT). Proposed in a paper named [\"Attention Is All You Need\"](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), the Transformer model is possibly the greatest user of the attention mechanism- it outperforms past models in accuracy while being *faster at fitting* than Recurrent and Convolutional Neural Networks. It entirely replaces the Recurrent Layer used in the Encoder-Decoder architecture with 'multi-headed self-attention,' or locally-computed attention vectors, allowing for parallelization of computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBdfDmBJLs7v"
      },
      "source": [
        "# Coding the Transformer for ASR\n",
        "\n",
        "**This implementation of the Transformer was borrowed and modified almost in its entirety from [Keras' example](https://keras.io/examples/audio/transformer_asr/). I plan to annotate and explain it entirely**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASE9hs1Qi3jZ"
      },
      "source": [
        "\n",
        "## Define the Transformer Input Layer\n",
        "\n",
        "When processing past target tokens for the decoder, we compute the sum of position embeddings and token embeddings.\n",
        "\n",
        "When processing audio features, we apply convolutional layers to downsample them (via convolution stides) and process local relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b7k3PN3i9LK"
      },
      "source": [
        "\n",
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Q_-1hBjAdm"
      },
      "source": [
        "### Transformer Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjIuJXHojHtx"
      },
      "source": [
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNTOa1gEjEOy"
      },
      "source": [
        "## Transformer Decoder Layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRl4-epKjFis"
      },
      "source": [
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.self_dropout = layers.Dropout(0.5)\n",
        "        self.enc_dropout = layers.Dropout(0.1)\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
        "\n",
        "        This prevents flow of information from future tokens to current token.\n",
        "        1's in the lower triangle, counting from the lower right corner.\n",
        "        \"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
        "        return ffn_out_norm\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXko38zjKN5"
      },
      "source": [
        "## Complete the Transformer model\n",
        "\n",
        "Our model takes audio spectrograms as inputs and predicts a sequence of characters. During training, we give the decoder the target character sequence shifted to the left as input. During inference, the decoder uses its own past predictions to predict the next token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY17aoGsjN5O"
      },
      "source": [
        "\n",
        "class Transformer(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qATDvJ6Wkr-t"
      },
      "source": [
        "## Callbacks to display predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOBrVq4_Op2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30eb97ff-0f09-4e01-e87b-2e47616ea051"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnFXkLgrO20U"
      },
      "source": [
        "MODELS_DIR = '/ASR_Trained_Models/'  # Where our model will be saved in Google Drive\n",
        "MODEL_NAME = 'our-transformer'\n",
        "\n",
        "dest_folder = \"/content/gdrive/MyDrive\" + MODELS_DIR"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbzGLyKJkjih"
      },
      "source": [
        "class DisplayOutputs(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self, batch, idx_to_token, target_start_token_idx=1, target_end_token_idx=2\n",
        "    ):\n",
        "        \"\"\"Displays a batch of outputs after every epoch\n",
        "\n",
        "        Args:\n",
        "            batch: A test batch containing the keys \"source\" and \"target\"\n",
        "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
        "            target_start_token_idx: A start token index in the target vocabulary\n",
        "            target_end_token_idx: An end token index in the target vocabulary\n",
        "        \"\"\"\n",
        "        self.batch = batch\n",
        "        self.target_start_token_idx = target_start_token_idx\n",
        "        self.target_end_token_idx = target_end_token_idx\n",
        "        self.idx_to_char = idx_to_token\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # if epoch % 5 != 0:\n",
        "        #     return\n",
        "        source = self.batch[\"source\"]\n",
        "        target = self.batch[\"target\"].numpy()\n",
        "        bs = tf.shape(source)[0]\n",
        "        preds = self.model.generate(source, self.target_start_token_idx)\n",
        "        preds = preds.numpy()\n",
        "        print(f\"epoch:      {epoch}\")\n",
        "        for i in range(bs):\n",
        "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
        "            prediction = \"\"\n",
        "            for idx in preds[i, :]:\n",
        "                prediction += self.idx_to_char[idx]\n",
        "                if idx == self.target_end_token_idx:\n",
        "                    break\n",
        "            print(f\"target:     {target_text.replace('-','')}\")\n",
        "            print(f\"prediction: {prediction}\\n\")\n",
        "        \n",
        "        # Save Model!\n",
        "        dest = \"/content/gdrive/MyDrive\" + MODELS_DIR + MODEL_NAME + f\"-{epoch}\"\n",
        "        model.save_weights(dest, overwrite=True)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS4hsfFmkzlW"
      },
      "source": [
        "## Learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKd-HU9Vkyxl"
      },
      "source": [
        "\n",
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        init_lr=0.00001,\n",
        "        lr_after_warmup=0.001,\n",
        "        final_lr=0.00001,\n",
        "        warmup_epochs=15,\n",
        "        decay_epochs=85,\n",
        "        steps_per_epoch=203,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.init_lr = init_lr\n",
        "        self.lr_after_warmup = lr_after_warmup\n",
        "        self.final_lr = final_lr\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def calculate_lr(self, epoch):\n",
        "        \"\"\" linear warm up - linear decay \"\"\"\n",
        "        warmup_lr = (\n",
        "            self.init_lr\n",
        "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
        "        )\n",
        "        decay_lr = tf.math.maximum(\n",
        "            self.final_lr,\n",
        "            self.lr_after_warmup\n",
        "            - (epoch - self.warmup_epochs)\n",
        "            * (self.lr_after_warmup - self.final_lr)\n",
        "            / (self.decay_epochs),\n",
        "        )\n",
        "        return tf.math.minimum(warmup_lr, decay_lr)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        epoch = step // self.steps_per_epoch\n",
        "        return self.calculate_lr(epoch)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh31mk_6k4P3"
      },
      "source": [
        "## Create & train the end-to-end model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnFigtc5EJQ4"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "batch = next(iter(test_ds))\n",
        "\n",
        "# The vocabulary to convert predicted indices into characters\n",
        "display_cb = DisplayOutputs(\n",
        "    batch, VOCAB, target_start_token_idx=2, target_end_token_idx=3\n",
        ")  # set the arguments as per vocabulary index for '<' and '>'\n",
        "\n",
        "model = Transformer(\n",
        "    num_hid=200,\n",
        "    num_head=2,\n",
        "    num_feed_forward=400,\n",
        "    target_maxlen=MAX_TEXT_LEN,\n",
        "    num_layers_enc=4,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=len(VOCAB),\n",
        ")\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, label_smoothing=0.1,\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "3TDfuahlk6BA",
        "outputId": "036bc6ae-63c3-4d10-98b1-4babf780c16d"
      },
      "source": [
        "\n",
        "learning_rate = CustomSchedule(\n",
        "    init_lr=0.00001,\n",
        "    lr_after_warmup=0.001,\n",
        "    final_lr=0.00001,\n",
        "    warmup_epochs=15,\n",
        "    decay_epochs=85,\n",
        "    steps_per_epoch=len(train_ds),\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "history = model.fit(train_ds, validation_data=test_ds, callbacks=[display_cb], epochs=EPOCHS)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "15/42 [=========>....................] - ETA: 17:58 - loss: nan"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a9422a3942d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisplay_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op5zjLX23u3O"
      },
      "source": [
        "VOCAB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FJD5RiMhqYMz"
      },
      "source": [
        "# References / Resources used\n",
        "\n",
        "### Overviews  \n",
        "[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)  \n",
        "[End-to-End Models](https://youtu.be/q67z7PTGRi8)   \n",
        "[Timeline of Architecture](https://www.youtube.com/watch?v=3MjIkWxXigM&)  \n",
        "[Machine Translation Vis](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)  \n",
        "[Audio Features](https://youtu.be/PPmNYwVbcts)   \n",
        "[Attention Mechanism Intuition](https://youtu.be/SysgYptB198)  \n",
        "[Comparison of Audio Features for ASR](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)  \n",
        "[HMMs vs NNs](https://stats.stackexchange.com/questions/282987/hidden-markov-model-vs-recurrent-neural-network)\n",
        "\n",
        "### Models\n",
        "[Attention mechanism: Neural Machine Translation by Jointly Learning to Align and Translate ](https://arxiv.org/pdf/1409.0473.pdf)  \n",
        "[Listen, Attend, and Spell](https://arxiv.org/abs/1508.01211)  \n",
        "[Transformers: Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)   \n",
        "\n",
        "### Code  \n",
        "[Listen, Attend, and Spell Implementation by 30stomercury](https://github.com/30stomercury/Automatic-Speech-Recognition)  \n",
        "[ASR Transformer by Apoorv Nandan](https://keras.io/examples/audio/transformer_asr/)  \n",
        "\n",
        "### More Readings\n",
        "https://towardsdatascience.com/recognizing-speech-commands-using-recurrent-neural-networks-with-attention-c2b2ba17c837  \n",
        "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDCvyHwywhif"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0Bynw3zFvJg"
      },
      "source": [
        "References:\n",
        "\n",
        "\n",
        "https://stats.stackexchange.com/questions/282987/hidden-markov-model-vs-recurrent-neural-network\n",
        "\n",
        "Jurafsky textbook"
      ]
    }
  ]
}
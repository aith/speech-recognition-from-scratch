{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook, I'll provide an overview on Automatic Speech Architecture, its standard pipelines, and how to train a model for it!\n",
    "\n",
    "### What is Automatic Speech Recognition?\n",
    "Automatic Speech Recognition (ASR) is the translation of spoken speech to text by a computer.\n",
    "\n",
    "It has many uses:\n",
    "+ closed captioning\n",
    "+ mobile phone voice assistants\n",
    "+ interface for handicapped individuals\n",
    "+ preserve endangered languages\n",
    "\n",
    "However, ASR is very difficult to solve due to:\n",
    "+ Environment (Is there noise? Other speakers?)\n",
    "+ Style of speech (Is it casual, formal, poetic, etc?)\n",
    "+ Style of speaker (Talks fast? Accents? Gender bias?)\n",
    "\n",
    "https://www.youtube.com/watch?v=q67z7PTGRi8\n",
    "\n",
    "Modern ASR models work quite well in ideal, noiseless conditions. However, in noisy conditions they are far from perfect. Below, we can see the typical accuracy for state-of-the-art ASR models in 2020!\n",
    "\n",
    "(Note: To calculate the accuracy of ASR models, we use a 'Word Error Rate.' This determines which percentage of spoken words were incorrectly translated to text by the ASR model. )\n",
    "\n",
    "![word error rate](https://drive.google.com/uc?export=view&id=1iUz9koIqQErBVB0a4db0gSZVdfsfwh9L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Architecture:\n",
    "\n",
    "![timeline](https://drive.google.com/uc?export=view&id=1n2ID2ML5x9TFSBJhfdvj5AGI4u-9Mp4m)\n",
    "\n",
    "\n",
    "From the 1980s to the 2010s, ASR was predominately performed by Hidden Markov Model (HMM) architecture. HMMs are probabilistic models for linear sequence classification problems. Given text's linear nature, HMMs seem like a natural fit for ASR.\n",
    "However, using HMMs for ASR come with 3 downsides:\n",
    "+ HMMs are built on the Markov Assumption, which assumes that the probability of the next state only depends on the current state, and not any states prior. This doesn't make sense for language processing, which is highly contextual (see Coarticulation)\n",
    "+ HMMs' state transitions probabilities are 'baked in' and thus inflexible to changes in language\n",
    "+ Classic HMM ASR pipelines require hand-tuned probability distributions for accoustic models and language models from linguistic experts\n",
    "+ HMM-based ASR architecture is very complex, requiring 3 models as input. The figure below demonstrates an HMM-centric ASR architecture\n",
    "\n",
    "![an HMM-based architecture](https://drive.google.com/uc?export=view&id=1loF8wbD-6DRO45Ly7lKnVVIcCn83H-B9)\n",
    "\n",
    "Nowadays Recurrent Neural Networks (RNNs) or RNN-HMM hybrids are favored for ASR because they solve these problems. However, RNN's come at the cost of requiring a vast amount of training data.\n",
    "\n",
    "![an RNN-only architecture](https://drive.google.com/uc?export=view&id=1_9McwMlHIqlPuJGYOkPwB-8h-t13P4pI)\n",
    "(an RNN-only model that has only 4 steps)\n",
    "\n",
    "https://stats.stackexchange.com/questions/282987/hidden-markov-model-vs-recurrent-neural-network\n",
    "Jurafsky textbook\n",
    "\n",
    "For the our ASR model, we'll be using an 'Listen, Attend, and Spell' (LAS) RNN. The LAS was proposed by in 2016 for ASR architecture in order to simplify Hybrid architecture (such as DNN-HMMs or CTC-HMMs) and avoid the aforementioned independence assumptions. Since then it has been the forefront of ASR architecture.\n",
    "\n",
    "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44926.pdf\n",
    "\n",
    "The objective of LAS is to transform an arbitrary-length input sequence to an arbitrary-length output sequence. LAS is an encoder-decoder model- a class of RNN's that is well-suited for problems where the length of the inputs greatly differs from the length of the outputs (like ASR).\n",
    "\n",
    "Encoder-Decoder models are composed of 3 parts:\n",
    "+ an Encoder\n",
    "+ a Context Vector\n",
    "+ a Decoder\n",
    "\n",
    "The Encoder compresses and summarizes input data into a Context Vector, which is later transformed by the Decoder. The Context Vector must be defined with a fixed-length (usually 256, 512, or 1024).\n",
    "\n",
    "Accoustic Information as Input for RNN:\n",
    "A classic HMM-based ASR model would require us to input 3 probabilistic models (accoustic, pronunciation, and language), whereas an RNN only requires 1 (accoustic). The accoustic model captures important characteristics about how we perceive speech (see MFCC). We'll extract this data from the LibriSpeech ASR dataset.\n",
    "\n",
    "https://www.openslr.org/12\n",
    "\n",
    "We need to first extract features from the audio files- we'll be putting these features in a vector and feeding them into the RNN. In order to extract the desired features, we first need to convert LibriSpeech's FLAC files into waveforms:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def flac_to_waveform(filepath):\n",
    "    sound_data, sample_rate = librosa.load(filepath)\n",
    "\n",
    "\n",
    "# - split audio into frames\n",
    "# - capture the features of each frame\n",
    "# - take the FLAC files from LibriSpeech\n",
    "# - turn it into a Waveform, quantized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we must obtain the frequency data across the time domain using a Fourier Transform. This is called a Spectrogram. However, human hearing is logarithmic; we are less sensitive to changes in high amplitude than we are to changes in low amplitude. To solve this we can map the Spectrogram to the Mel Scale, and then take the log of the result.\n",
    "\n",
    "code:\n",
    "- librosa.features.melspectrogram func\n",
    "- librosa.logamplitude\n",
    "- flatten\n",
    "-\n",
    "\n",
    "https://www.kdnuggets.com/2017/12/audio-classifier-deep-neural-networks.html\n",
    "\n",
    "Next: build encoder-decoder architecture, based on https://github.com/tensorflow/nmt\n",
    "\n",
    "Next: plug into RNN, run it on Google Colab. Save it\n",
    "\n",
    "Next: let user input voice data, and run it through the model\n",
    "\n",
    "More soucres\n",
    "https://towardsdatascience.com/recognizing-speech-commands-using-recurrent-neural-networks-with-attention-c2b2ba17c837\n",
    "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "https://arxiv.org/pdf/1409.0473.pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Further Optimizations:\n",
    "\n",
    "The Context Vector being fixed-length turns out to be a performance bottleneck; given a long input sentences, the the Encoder may not be able to store all of its output in the Context Vector in one timestep.\n",
    "\n",
    "To solve the bottleneck, we could use the 'attention' mechanism which involves the Decoder selecting from all hidden states provided by the Encoder. For the sake of simplicity we won't be doing this optimization.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}